# Risk Assessment Algorithms in the Criminal Justice System - An Ethical Investigation
### Khadija & Maddy


**1. Introduction**

Risk assessment algorithms are a tool used by the courts, usually judges and prosecutors, that utilize neural networks to predict the probability of a defendant repeating criminal activity based on a selection of individual attributes (demographics, criminal history, etc.). Although these algorithms are an attempt to minimize personal and institutional bias in the judicial system, the algorithms present a number of ethical concerns regarding their inputs and proper role in society. This paper evaluates the effectiveness and ethical questions raised by real-world algorithm applications developed for risk assessment in the criminal justice space. 


**2. Literature Review**

*Predicting Enemies* by Ashley S. Deeks and *Understanding Risk Assessment Instruments in Criminal Justice* by Alex Chohlas-Wood are articles that provide a detailed analysis of the different ways in which the predictive algorithms are used in the criminal justice system. Chohlas-Wood goes into detail on what tools are currently being used and what companies are creating them. How these algorithms could be fixed for the better is an important part of this article, whereas Deeks’ goal in Predicting Enemies is to warn off other sectors from utilizing these kinds of algorithms. Deeks elaborates on how the military industrial complex is working to adopt similar tactics in their work, but emphasizes that moving forward in this direction would be a mistake because of issues already present in their utilization in the criminal justice system. In a similar vein *The Report on Algorithmic Risk Assessment Tool in US Criminal Justice System* published by the nonprofit Partnership on AI reports on the serious shortcomings of risk assessment tools in the criminal justice systems through consultations with experts and a review of the literature on risk assessment tools.

Work has also been done on how the discretionary use of risk assessment algorithms, rather than strictly adhered to policy, can add bias to the very proceedings these algorithms were aimed to neutralize. In *Predictive Algorithms and Criminal Sentencing*, Angele Christi analyzes how the criminal justice system relies on algorithms to determine sentences and criminality while performing an in-depth analysis of the ways in which judges and prosecutors use these algorithms to suit their interests in court, utilizing the results when they want and disregarding them when they do not.

**3. Purpose of Risk Assessment Algorithms**

Risk assessment algorithms and instruments come with the intention of using data to intelligently make decisions regarding a defendant’s risk factors in the judicial system and thereby standardize these decisions across the courts. In their idealized form “algorithmic RAIs have the potential to bring consistency, accuracy, and transparency to judicial decisions.”(Brookings, 2020). Such achievements would be an important step in long-sought criminal justice reforms for the United States. An analogy drawn by advocates is with the transformation seen in professional baseball, that we need to “moneyballing justice” (Christin, 2019). If successfully applied these algorithms could reduce bias and mitigate unnecessary incarceration.

The current state-of-the-art in risk assessment algorithms being deployed in policing and criminal justice sectors includes the Public Safety Assessment (PSA) system. The Public Safety Assessment algorithm (PSA) is used by judges to determine pretrial release risk. A defendant’s biographical information and criminal history are used to assign 3 risk scores - that they will be convicted for a new crime, that they will be convicted for a new violent crime, and that they will fail to appear in court. These scores are then used by a judge or prosecutor to determine whether a defendant can await trial at home or in custody.

**4. Methods and Inputs**

Throughout the country dozens of algorithms are used by dozens of jurisdictions, “including Arizona, Kentucky, New Jersey, Charlotte, Chicago, and Phoenix” (Deeks, 2018). Each of these RAIs use slightly different metrics to gauge the recidivism risk of an individual. Although the internal designs of these algorithms are kept confidential, the public is privy to some of the inputs that are factored into the recommendations the RAIs make. In Chicago the Strategic Subject list takes into account the age of the defendant, whether they have been the victim of an assault and battery or shooting, and the defendant’s arrest and conviction records” (Deeks, 2018). The Level of Service Inventory-Revised algorithm uses a defendant’s answers to questions concerning their “criminal history, education, employment, financial problems, family or marital situation, housing, hobbies, friends, alcohol and drug use, emotional or mental health issues, and attitudes about crime and supervision” (Deeks, 2018). In contrast the popular HunchLab algorithm “primarily surveys past crimes, but also digs into dozens of other factors like population density; census data; the locations of bars, churches, schools, and transportation hubs; schedules for home games— even moon phases” (Deeks, 2018)


**5. Results**

Results on how effectively RAIs predict a defendant’s risk vary. A 2012-2014 study in Virginia showed positive impacts in both pretrial misconduct and pretrial incarceration rates, while a North Carolina study on the implementation of PSA found only a positive impact on pretrial incarceration rates while pretrial misconduct rates went unchanged (Brookings, 2020). However, a third study in Kentucky found little reduction in incarceration rates in the 2009-2016 period after RAIs were put into use (Brookings, 2020). Interestingly this same study found that “a judge’s use of an RAI did not unevenly impact outcomes across race groups” (Brookings, 2020).

**6. Ethical Concerns**

While high levels of accuracy point to some of these algorithms being fairly accurate predictors of recidivism in the real-world, strong prediction capabilities do not indicate whether use of the algorithm to decide protocols in the real world is ethically justified. Brookings Institute identifies four main areas of concern that critics have used to argue against RAIs, “their lack of individualization, absence of transparency under trade-secret claims, possibility of bias, and questions of their true impact”(Brookings, 2020), all of which are examined in this paper. An ethical framework for evaluating RAI algorithms is presented by the Partnership on Artificial Intelligence in the form of four baselines. “Do risk assessment tools achieve absolute fairness? Are risk assessment tools as fair as they can possibly be based on available datasets? Are risk assessment tools an improvement over current processes and human decision-makers? Are risk assessment tools an improvement over other possible reforms to the criminal justice system?”(PAI). These questions can help to situate the comparative help and harm RAI algorithms can have on the criminal justice system in the United States.

**6a. Individuality**

The lack of individuality that comes with a decision from an algorithm was the main argument used in the most influential legal case regarding the use of risk assessment algorithms, the 2016 case Loomis v. Wisconsin. Loomis contended that the use of the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool meant his sentencing was not individualized, but rather “informed by historical group tendencies for misconduct, as assessed by COMPAS” (Brookings, 2020). Although Loomis lost his case, the Brookings Institute makes an important note that “both humans and algorithms learn from historical behavior [...] a risk prediction for a given individual—whether from a judge or an RAI—is, as a result, anchored in the historical behavior of similar individuals” (Brookings, 2020). This assertion begs the question of whether it is ethical and allowable to make legal decisions for an individual based on the behavior of others in their group. The Partnership on Artificial Intelligence notes that “making predictions about individuals from group-level data is known as the ecological fallacy [...] although risk assessment tools use data about an individual as inputs, the relationship between these inputs and the predicted outcome is determined by patterns in training data about other people’s behavior”(PAI).

While not a risk assessment tool used by the courts, the Beware software used for policing illustrates the potential danger of this fallacy. Based on the concept of “negative social networks” encouraging criminal activity, Beware assigns threat scores and levels to any given person, area, or address that a police officer enters (Deeks, 2018). It is controversial in the extreme because “it focuses police attention on individuals who may not have actually committed an offense”(Deeks, 2018) based on historical data about their group.

**6b. Transparency**

Christin attributes the lack of transparency to three main sources. First, the risk-assessment algorithms used by courts and police departments are proprietary products created by private companies such as Palantir. In order to protect their product from imitation, these companies do not reveal specifications on the design, input types, and training data. The second source Christin identified was the barrier to understanding the code for the layman. Even if the source code for an algorithm was publicly available, how that code worked to create a decision is not easily accessible for most defendants. Furthermore, Christin contends that the objectivity of the decision maker is compromised by this lack of understanding of the operation of the algorithm as they tend to take the result as a hard fact. An important distinction to make is that all of these results are predictions, not statistics. Finally, because these algorithms are neural networks and not hard-coded conditions, even the programmers of the algorithms cannot determine exactly why an algorithm made the decision it did. This is known as black boxing, where “important social, political, and ethical questions about sentencing decisions are not asked, because no one knows how the algorithm works” (Christin, 2019).

**6c. Bias**

Many of these algorithms rely on historical recidivism data to inform their decisions. The problem with historical data, specifically as it pertains to demographic groups, is how it has been influenced by unequal institutional systems. When the inputs to an algorithm are biased the outputs are sure to display the same biases regardless of the intent of the designers or users. Unlike in other areas of AI, issues of bias cannot easily be ascribed to the dataset used for training because the inputs to these algorithms will always be biased. Instead many question whether these algorithms should be developed in the first place because of this inherent shortcoming. Although designed with a primary goal of reducing human bias, “algorithms tend to reinforce social and racial inequalities instead of reducing them” (Christin, 2019).

Not much data has been collected on how RAIs impact bias in the judicial system. The most notable was an incendiary 2016 study by ProPublica on how COMPAS predictions compared to actual recidivism data taken 2 years later in Florida. The article’s findings were that COMPAS showed racial bias because it had more false positives for Black individuals than for white individuals. It should be noted that this result did not account for the “differences in underlying offense rates for each race”(Brookings, 2020). When examining “whether individuals with the same risk score re-offend at the same rate, regardless of race—evidence of racial discrimination disappears”(Brookings, 2020). 

**6d. Human Oversight**

The need for human oversight of these algorithms is of primary concern when looking for bias in the real-world judicial system. Most people concede that “policymakers should preserve human oversight and careful discretion when implementing machine learning algorithms [...] it is always possible that unusual factors could affect an individual’s likelihood of misconduct [so] a judge must retain the ability to overrule an RAI’s recommendations”(Brookings, 2020). While this helps to rectify the lack of individuality covered earlier in this paper, it introduces a large avenue for the judge or prosecutor’s personal bias. If it is at the discretion of the judge whether or not follow the RAI’s recommendation, it can no longer be claimed that human biases don’t impact the result for the defendant.

**7. Conclusion**

This paper presented a number of ethical critiques regarding the use of risk assessment algorithms in the criminal justice system. Most of these concerns still lead to unresolved questions that society needs to tackle. Is it possible to create neural networks that resist the unjust biases of modern society? Do algorithms even have a place in determining justice, or are these decisions best left to human minds capable of empathy and compassion despite their biases?
