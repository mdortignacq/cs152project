**Introduction**

Risk assessment algorithms are tools used by the courts, usually judges and prosecutors, that utilize algorithms such as deep learning networks to predict the probability of a defendant repeating criminal activity based on a selection of individual attributes (demographics, criminal history, etc.). 
Let us give background to how risk assessment algorithms came to be. This method of  crime/risk prediction is not new in the justice system: it has been used in the criminal justice system since the 1930s. The work was done through clinical inquiry done by an expert in the field, primarily psychologists and social workers rather than an algorithm. In the late 60s and 70s, the research was focused on identifying dangerousness, but turned out to be very inaccurate by mistakenly identifying more than 50% of individuals as dangerous. Regardless of this method proving time and time again that trying to predict the criminality or risk of a person was futile, it has been continuously used in the criminal justice system. There was a period in the 70s and 80s when there was a shift to individual sentencing, which focused on punishment based on an individual's characteristics instead of just their crimes, with the goal of rehabilitation. To guarantee that there was individual treatment, judges were allowed immense discretion on sentencing. This sentencing discretion caused several issues as unsurprisingly, minorities were disproportionately treated more harshly than their white counterparts. This shows us that even when using rehabilitative methods, there were still discriminatory implications. 
After this, there was a shift back to sentences based primarily on the crime rather than the individual, which meant that there were much clearer sentencing practices and guidelines. Though the rehabilitative method was discriminatory, the return to crime-focused sentencing was a mass incarcerating monster and even more discriminatory than the former. What is the trend here? Using evidence, data and scientific methods to criminalize someone. In doing this you are criminalizing one's identity before they are even arrested.  These evidence and data based prediction practices will never not discriminate because of who is considered “criminal”, “risk”, “dangerous” in our criminal justice system. 
This is the foundation of risk assessment algorithms used in court today: they are an iteration of these evidence-based scientific methods already known to incriminate and discriminate against minorities. Now, rather than a worker in the criminal justice system deciding who is more of a risk or not we rely on risk assessment algorithms. These may be even more detrimental than the psychologist or social worker, as there is more trust in technology, there is barely any regulation for this technology, and the technology is used at the discernment of the judge.The introduction of these algorithms are posed as an attempt to minimize personal and institutional bias in the judicial system, but the algorithms present a number of ethical concerns regarding their inputs, role and effects in the criminal justice system. In this ethical investigation we will explore and explain how risk assessment algorithms are  detrimental to the criminal justice system, exacerbate the bias and discrimination found in the criminal justice system and cannot be unbiased or useful in this space by nature of their origin and use. Our goal here is to show how the work done by computer scientists and others in the related field are not distinct from the world and have serious social implications. We need to be very thorough in making our algorithms, neural networks, devices and such to ensure that they are useful and necessary wherever they are being used and not causing or contributing significantly to social, andethical issues. 

Related Work
As described in the intro, even before the risk assessment algorithms the prediction of risk/crime has been a method used in the criminal justice system for at least a century, though there was not much study or interest in its early days. The current use of the method by risk assessment algorithms has stirred up quite a variety of literature detailing how police and the courts are silently and increasingly using these algorithms to make decisions. We will be describing a few relevant sources below to give us more historical context and grounding in the topic. These works  all criticise risk assessment in some ways but see some merit to having them in the justice system, unlike our work here where we are clearly stating that no good comes from them being used. 
Predicting Enemies by Ashley S. Deeks and  Predictive Algorithms and Criminal Sentencing by Angele Christin are two articles that are very similar in their approach and conclusion onf the topic. They provide a detailed analysis of the different ways in which the predictive algorithms are used in the criminal justice system. A common thread between the 2 articles were their arguments about the biases present in the use of risk assessment algorithms by emphasizing how these algorithms are already using biased data to make decisions. Deeks also emphasizes how human perception of these algorithms cause them to be more detrimental as there is a blind trust in them. This is problematic, as judges and prosecutors use these algorithms to suit their interests in court, utilizing the results when it suits their judgement and disregarding them when they do not.
Another point that these 2 works highlight is the opacity in the use of these algorithms. Christin describes how it is difficult for one to know how an algorithm assessed a case nor can a person or entity be held accountable if the algorithm made a mistake. Christin also explains how risk assessment algorithms are proprietary products, meaning that the companies who create them do not allow sharing of their algorithms or training sets, for fear of imitation. There is also the issue that most people in the justice system cannot read or write any forms of code, which makes it difficult to interpret or use it as evidence even if it is accessed. These readings give a thorough look into how risk assessment algorithms are  detrimental when used in the justice system. That said, the sentiments they express at the end of their analysis still shows hope in reforms being able to fix these issues. 
Another article that gives us more insight into risk assessment algorithms is Understanding Risk Assessment Instruments in Criminal Justice by Alex Chohlas-Wood. Chohlas-Wood goes into detail on what tools are currently being used and what companies are creating them. This article, unlike the 2 highlighted above, more thoroughly describes what is actually used as criteria for these risk assessment algorithms. “An RAI (risk assessment instrument) called the Public Safety Assessment (PSA) considers an individual's age and history of conduct, along with other factors to produce 3 differen risk scores: the risk that they will be convicted for any new crime, the risk that they will be convicted for a new violent crime, and the risk that they will fail to appear in court”. From this quote,we have a better understanding of what criteria is used for the algorithms and what they are used to predict. Though this article highlights various arguments detailing the discrimnation that RAI’s perpetrate, it also gives recommendations to help decrease the issues and concerns that RAI’s impose. This is different from the 2 articles described above that just give hopeful sentiments and warning sectors away from using these kinds of algorithms.
In a similar vein, The Report on Algorithmic Risk Assessment Tool in the US Criminal Justice System published by the nonprofit Partnership on AI reports on the serious shortcomings of risk assessment tools in the criminal justice systems through consultations with experts and a review of the literature on risk assessment tools. The report highlights concerns about RAIs in terms of the accuracy and bias of the tools, issues with the interface, the humans who use them, and questions of transparency and accountability. They provide a very detailed analysis of the technical concerns of the algorithms, describing how they may amplify the pitfalls of human decision making in trying to mitigate it. Their main goal is to aid policy makers in making more informed decisions on risk assessment tools and provide a few suggestions in order to do that.
All these articles and research describe very vividly how risk assessment algorithms are used and abused in courts. They highlight the different effects they have and the concerns and issues associated with them. All of these articles, though critical in their analysis of risk assessment algorithms, still have a hope in their reform and ability to be used for good in the criminal justice system. Our ethical analysis will show how this is not necessarily possible due to the nature of risk assessment algorithms and their specific role in the prison industrial complex. 

**Purpose of Risk Assessment Algorithms**
The very issue with risk assessment algorithms and why they will always be detrimental  lies within their intended purpose. As explained in the introduction, the algorithm itself came about to replace the work done by people who were assessing the risk of an individual. That work was already controversial as it was known to target minorities. If we look at contemporary descriptions of the intentions of risk assessment algorithms they will most likely be described as follows: Risk assessment algorithms are an instrument intending to use data to intelligently make decisions regarding a defendant’s risk factors in the judicial system and thereby standardize these decisions across the courts. In their idealized form “algorithmic RAIs have the potential to bring consistency, accuracy, and transparency to judicial decisions.”(Brookings, 2020). In plain terms, this just means that it is a tool used to determine the criminality, dangerousness and risk of an individual. If we explore what that means in the context of the US prison industrial complex it becomes very clear that the risk assessment algorithms are working exactly as designed. 
There has been extensive dialogue, research and theorization of the prison industrial complex and its effect of criminalizing and pumping of Black and Latinx people into prison. This is done in the name of keeping the streets safe, as they put millions of people in prison who oftentimes have committed  “crimes” due to the structural issues already present in the US, such as poverty, institutional racism and other social issues. This causes crime to have a face:s usually a Black one. When an algorithm is introduced to this already racialized and discriminatory environment it will emulate it. What is particularly abhorrent about using algorithms in these spaces is that they  use characteristics about an individual to determine their risk and if the ideas of risk and criminality themselves are already entrenched in racist and white supremacist ideals, what else would the algorithm provide as a result? Let us look at an example. 
The current state-of-the-art in risk assessment algorithms being deployed in policing and criminal justice sectors includes the Public Safety Assessment (PSA) system. The PSA is used by judges to determine pretrial release risk. A defendant’s biographical information (area, financial information, family status ...) and criminal history are used to assign 3 risk scores - the likelihood that they will be convicted for a new crime, the likelihood that they will be convicted for a new violent crime, and the likelihood that they will fail to appear in court. These scores are then used by a judge or prosecutor to determine whether a defendant can await trial at home or in custody. The main point is that despite the algorithms, these criteria are already racialized, the data that would be used is already biased. The prison industrial complex is already known to target certain groups, specifically Black and Latinx communities. All that these risk assessment algorithms would do is reify and strengthen the biases present. Humans are known to make mistakes and be biased, but in terms of technology, there is a trust and false narrative that they are supposed to be more factual and objective. This is clear in the intention as shown by the quote by Brookings that says RAIs have potential to be more consistent and accurate. 
The main misconception about RAIs is that they are often making mistakes in their decisions and being biased against minorities. They are not, they are precisely doing what they are intended to, those mistakes are replicating exactly the mistakes that the courts often make in regards to being biased and discriminatory.  The issue then lies in the foundation of these tools, it lies in the structural issues steeped in the criminal justice system. Blaming the risk assessment algorithms is just a buffer to the real issue at hand because we know that before the algorithms, this work was commonplace in courts. The algorithms in fact exacerbate these structural issues by arming the prison industrial complex with a tool that is difficult to dispute since it is supposed to be logical, it is supposed to be accurate. They increase the power and validity of these structures and we can see that clearly in the way a judge can use them to validate and give reason to their judgment and also be cast aside by the judge if not satisfactory to their ruling. They solidify the racial and white supremacist ideas by proving to the world with yet another reason that these Black folks belong in jail becauseeven the computer says that theyare dangerous and risky. As we have explained, the articles we have found push for reforms and indulge in the idea that there is a way to fix this issue when, as the algorithm is concerned there is no issue. It is fulfilling its duties very well. There is no way to reform or successfully use the RAIs to not be discriminatory or biased. 
There is no way RAIs can be used in the criminal justice system without them reifying and exacerbating the same issues they are meant to “reduce”. We will continue showing this in our methods section by detailing  the technical and data aspects that will help us understand how they seem to work flawlessly to repeat the discrimination against Black and Brown folks in the prison industrial complex. 

**Methods and Inputs**

We will now explore the known methods and inputs that pertain to risk assessment algorithms. One of the issues that we have already mentioned about RAIs is the lack of transparency about their design and development. These details are kept confidential in the name of protecting the companies’ rights to their code and methods, so it is even more difficult to refute them because of that fact. We will use limited information we have to get a sense of how exactly these algorithms would work and show how they are working as intended. 
Throughout the country various algorithms are used by dozens of jurisdictions, “including Arizona, Kentucky, New Jersey, Charlotte, Chicago, and Phoenix” (Deeks, 2018). Each of these RAIs use slightly different metrics most likely different variations of personal biographical and characteristic information to gauge the recidivism risk of an individual. Although the internal designs of these algorithms are kept confidential, the public is privy to some of the inputs that are factored into the recommendations the RAIs make. These recommendations are often biased and discriminatory but that will be elaborated on in the results section. In Chicago the Strategic Subject list takes into account the age of the defendant, whether they have been the victim of an assault and battery or shooting, and the defendant’s arrest and conviction records” (Deeks, 2018). The Level of Service Inventory-Revised algorithm uses a defendant’s answers to questions concerning their “criminal history, education, employment, financial problems, family or marital situation, housing, hobbies, friends, alcohol and drug use, emotional or mental health issues, and attitudes about crime and supervision” (Deeks, 2018). Just looking at those criteria, they already point to a racialized and classed individual who would most likely have these experiences. This data already identifies a “criminal” for the algorithm before even being trained by neural networks or other ML methods that may be used for RAIs. We must interrogate these categories: Who is most likely to already have a criminal history? Who is likely to have high education qualifications? What groups of people are more likely to have lower credit scores and more financial issues? Every single one of those categories targets certain groups of people impacted by structural and social issues that narrate the state of their lives. The prison industrial complex is one of these structures. What precisely would stop the algorithms from doing the work of the prison industrial complex as ? Nothing, the algorithm would produce the same result the system does.  
In contrast to the previous criteria, the popular HunchLab algorithm “primarily surveys past crimes, but also digs into dozens of other factors like population density; census data; the locations of bars, churches, schools, and transportation hubs; schedules for home games— even moon phases” (Deeks, 2018). Though the use of these inputs seem less blatantly biased, it has the same effect. There is a gigantic overrepresentation of Black people in prisons so most surveys of past crimes will have an overrepresentation of Black people. There are a numbern of structural, historical and political reasons why Black and Latinx neighborhoods are the way they are. From the lack of resources, to the ruining of their flourishing communities, to the taking of political power, the gentrification and countless issues that cause more Black and Latinx people to fall directly into these categories. This here shows how identities are criminalized and how the algorithms will always come to this conclusion when given data because it is these attributes that make the criminal not the crime. 
Let us consider what we learned earlier in our course about the nature of neural networks to give yet another example of how no matter how these algorithms are manipulated they will continue to be detrimental in the criminal justice system. From the literature we have, it seems that most risk assessment algorithms use a deep learning approach. A neural network takes in a large dataset or inputs and then develops a system with which to learn from that data. The more data you have the more accurate it gets. For example in class we worked greatly with the MNIST dataset of handwritten digits to train out NN to recognize the digits. For our situation let us say the inputs are from the example above: criminal history, education, employment, financial problems, family or marital situation, housing, hobbies, friends, alcohol and drug use, emotional or mental health issues and attitudes about crime and supervision. If we were to give this to our neural networks and have it learn based on these characteristics who is most likely to be a risk based on existing data of criminals. it would be able to help us predict who is likely to commit a crime, commit a crime again and other such questions. The way in which this neural network would work can be manipulated and tweaked in a number of different ways depending on what the person making it factors and focuses on. This clarifies how a neural network is not separated at all from the person making it and the data that is being used to train it. Depending on the definition of a criminal, it can give a very accurate determination of how criminals are defined in the justice system. They may very well be doing their job, despite popular belief and are simply not supposed to be used in these types of environments because all that will result in is a regurgitation of issues within the prison industrial complex. 

**Results**

We will now move to detailing how the results from these methods further solidify what we discuss in this ethical investigation. From the methods outlined above the neural network should give predictions on the criminality of an individual. Continuing the example from our previous section in regards to a neural network, let compare what results we may get if we give the neural network the data of an African American male who does not have a criminal record, dropped out of high school, is unemployed, is housing insecure and has friends with criminal history with a white male who does have a criminal record, is college educated, employed, has a mortgage and good credit. Now depending on the weights on each input, the results could vary but we can infer that the prediction would most likely categorize the first individual as more criminal or risky just by the circumstances of their life. However, the one who has actually committed an offence seems much more  innocent based on the power and money they possess. This criminalizes racialized poor folk just by their existence alone, no crime done. This uplifts and makes innocent those who have money, resources and power and by this criteria the algorithm will be biased against poor minorities by nature of this goal of characterizing a criminal.  
The article How We Analyzed the COMPAS Recidivism Algorithm by Jeff Larson, Surya Mattu, Lauren Kichner and Julia Anagin gives us more results to inspect RAI’s. They found that “Black defendants were far more likely than white defendants to be incorrectly judged to be at a higher risk of recidivism while white defendants were more likely than Black defendants to be incorrectly flagged as low risk.” This describes exactly what we outlined in the neural network example above, what is considered criminal in the justice system is not directly correlated to crimes done but to characteristics that are “supposed” to be criminal under white supremacy. When comparing the recidivism risk categories predicted by “COMPAS and the actual recidivism rates, their analysis found the following: 
“Black defendants were often predicted to be at a higher risk of recidivism than they actually were. Our analysis found that black defendants who did not recidivate over a two-year period were nearly twice as likely to be misclassified as higher risk compared to their white counterparts (45 percent vs. 23 percent).”
“White defendants were often predicted to be less risky than they were. Our analysis found that white defendants who re-offended within the next two years were mistakenly labeled low risk almost twice as often as black re-offenders (48 percent vs. 28 percent).”
“The analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 45 percent more likely to be assigned higher risk scores than white defendants.”
“Black defendants were also twice as likely as white defendants to be misclassified as being a higher risk of violent recidivism. And white violent recidivists were 63 percent more likely to have been misclassified as a low risk of violent recidivism, compared with black violent recidivists.”
“The violent recidivism analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 77 percent more likely to be assigned higher risk scores than white defendants.”(Larson, Mattu, Kirchener, Angwin, 2016) 
These results are abominable and show just how biased, discriminatory and racist these algorithms are. It is because they are replicating the biased, discriminatory and racist prison industrial complex. The algorithms may be doing an even better job than those in the role before them as they are being trained over and over to make these predictions. 

**Ethical Concerns**

While high levels of accuracy point to some of these algorithms being fairly accurate predictors of recidivism in the real-world, strong prediction capabilities do not indicate whether use of the algorithm to decide protocols in the real world is ethically justified. Brookings Institute identifies four main areas of concern that critics have used to argue against RAIs, “their lack of individualization, absence of transparency under trade-secret claims, possibility of bias, and questions of their true impact”(Brookings, 2020), all of which are examined in this paper. An ethical framework for evaluating RAI algorithms is presented by the Partnership on Artificial Intelligence in the form of four baselines. “Do risk assessment tools achieve absolute fairness? Are risk assessment tools as fair as they can possibly be based on available datasets? Are risk assessment tools an improvement over current processes and human decision-makers? Are risk assessment tools an improvement over other possible reforms to the criminal justice system?”(PAI). 
To answer these questions as shown in our ethical investigation: 
No, they do not achieve absolute fairness and never will be because of their original intention and inherent bias that the data used for them have. These algorithms will always be biased and discriminatory by their very nature. 
This question is a misnomer because the datasets and whole system is already unfair; there is no way to attribute any type of fairness with risk assessment algorithms. They are only fair for the criminal justice system to justify their incarceration of minorities. 
They are not an improvement and are even more detrimental and exacerbate the issues present within the prison industrial complex. The previous work humans were doing were as bad but there was not this factor of blind trust in them as there is in the algorithms. 
They are not an improvement at all and actually distract from the systemic work that needs to be done to actually address the issue that they are placing on risk assessment algorithms. 
In the following subsections we will evaluate other concerns that are associated with RAIs and provide more structure to why they need not exist in decision-making processes in the criminal justice system. 

**6a. Individuality**

The lack of individuality that comes with a decision from an algorithm was the main argument used in the most influential legal case regarding the use of risk assessment algorithms, the 2016 case Loomis v. Wisconsin. Loomis contended that the use of the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool meant his sentencing was not individualized, but rather “informed by historical group tendencies for misconduct, as assessed by COMPAS” (Brookings, 2020). This is the phenomenon we established in our results.  Although Loomis lost his case, the Brookings Institute makes an important note that “both humans and algorithms learn from historical behavior [...] a risk prediction for a given individual—whether from a judge or an RAI—is, as a result, anchored in the historical behavior of similar individuals” (Brookings, 2020). This assertion begs the question of whether it is ethical and allowable to make legal decisions for an individual based on the behavior of others in their group. We have shown that it is indeed a racist and discriminatory function of the criminal justice system being reiterated through RAIs. The Partnership on Artificial Intelligence notes that “making predictions about individuals from group-level data is known as the ecological fallacy [...] although risk assessment tools use data about an individual as inputs, the relationship between these inputs and the predicted outcome is determined by patterns in training data about other people’s behavior”(PAI). What this tells us is that whether through a person or an algorithm the prison industrial complex targets specific groups’ particularly Black and Latinx groups’ behavior as criminal and punishes them accordingly. Risk assessment algorithms are just a means of the greater goal of prisons: to incarcerate more minority groups.  
While not a risk assessment tool used by the courts, the Beware software used for policing illustrates the potential danger of this fallacy. Based on the concept of “negative social networks” encouraging criminal activity, Beware assigns threat scores and levels to any given person, area, or address that a police officer enters (Deeks, 2018). It is controversial in the extreme because “it focuses police attention on individuals who may not have actually committed an offense”(Deeks, 2018) based on historical data about their group. So this translates to increased policing of certain areas that are probably majority Black and this leads to increased arrests of Black people as police are patrolling the areas for crime which then leads to more Black people in prison. This causes a cycle of imprisonment for people in heavily policed areas.  Let's say police are patrolling an area and arrest someone who then goes to prison, they are released and most likely go back to that heavily policed area and end up in prison again, now when tried, that first time they were arrested will be by an algorithm to consider them a higher risk and used to incarcerate them for longer. Then if/once released they go back to that same area, the cycle continues. This method of grouping people and judging one based on racist group ideas just further entrenches minorities in criminal justice and continues to give reasons for it being necessary.    

**6b. Transparency**

Christin attributes the lack of transparency to three main sources. First, the risk-assessment algorithms used by courts and police departments are proprietary products created by private companies such as Palantir. In order to protect their product from imitation, these companies do not reveal specifications on the design, input types, and training data. This excuse makes it harder to access the probable plethora of evidence showing how unethical and unfair the algorithms they build are. This is also a way to protect them from lawsuits defendants most likely have against their products, since times defendants often lose these cases. The second source Christin identified was the barrier to understanding the code for the layman. Even if the source code for an algorithm was publicly available, how that code worked to create a decision is not easily accessible or accessible at all for most defendants. Nor are their lawyers equipped with the expertise to understand and refute what is happening in these algorithms. Even an expert would have some difficulty in understanding the code as they are many layers and complications involved in the code. Besides the blatant racist and discriminatory practices of these algorithms the barrier of most of the people in the courts not able to understand the code should be enough for it to not be a critical source to a case.  
Furthermore, Christin contends that the objectivity of the decision maker is compromised by this lack of understanding of the operation of the algorithm, as they tend to take the result as a hard fact. An important distinction to make is that all of these results are predictions, not statistics. The blind trust phenomenon with technology causes this to happen. Courts often use it to validate their rulings that probably have biases present. While it is true that there is the blind trust, on the flip side if the judge does not align with prediction of the algorithm they go off their own jurisdiction. This causes a double edged phenomenon where no matter their result, RAIs are used to the advantage and discretion of the judge to validate their ruling. 
Finally, because these algorithms are neural networks and not hard-coded conditions, even the programmers of the algorithms cannot determine exactly why an algorithm made the decision it did. This is known as black boxing, where “important social, political, and ethical questions about sentencing decisions are not asked, because no one knows how the algorithm works” (Christin, 2019). We often saw this phenomena happen in class, though we create and feed them the data, our design has the algorithms to teach themselves which causes some results to be unpredictable at times . If the expert and one who created that algorithm cannot explain a result how is it valid enough to use in court. 

**6d. Human Oversight**

The need for human oversight of these algorithms is a primary concern when looking for bias in the real-world judicial system. Most people concede that “policymakers should preserve human oversight and careful discretion when implementing machine learning algorithms [...] it is always possible that unusual factors could affect an individual’s likelihood of misconduct [so] a judge must retain the ability to overrule an RAI’s recommendations”(Brookings, 2020). While this helps to rectify the lack of individuality covered earlier in this paper, there is the fact that it was humans doing this work in the first place. Nothing would change in having human oversight, as the issue comes from those same people. This is a structural issue being clouded as a technological glitch. We also discussed how much of the decision is at the discretion of the judge, so the biases of a judge can overtake that of an RAI and the human oversight associated with. 

**Conclusion**

In this ethical investigation, we analyzed and critiqued the use of risk assessment algorithms in the criminal justice system. The history and foundation of risk assessment algorithms give reason to the issues they are causing today as they are emulating humans who have done this work of determining the risk factor, dangerousness of an individual. This method and way of analyzing crime is extremely racialized  and classist as it relies on characterizing criminals by characteristics. These characteristics are usually attributed to racialized poor folks and thus they are overrepresented and misrepresented in these methods. Make no mistake, the predictions the algorithms are producing are no accident, they are working exactly as they should. They are just telling of the horrors of the prison industrial complex in which risk assessment algorithms are just a tool. The prison industrial complex is increasingly shoving Black and Brown people into their prisons and these types of algorithms have no place there. Besides reifying and justifying the increased incarceration of those already overrepresented in prisons, they also take away from the necessary structural change that needs in relation to prison. They steal attention, time and effort that would have been used to organize around issues related to the prison industrial complex by having people discuss various reforms, tweaks and changes to make it work. They just need to be obsolete in this situation, as no matter what reform is done to them, they will continue to be detrimental and further incarcerate folks as this is the basis of their creation.
As people who are creating, innovating and working with technology we need to be very wary and knowledgeable about not only the technical aspects of what we do but also the impact of what we are making. We need to ask ourselves these types of questions:  “Is this tool actually useful?” “Is this going to exacerbate an issue already present?” “Is this even a relevant and applicable solution?” “How will this impact certain communities”. If not, all we are doing are creating technological monsters that will never be useful and cause great harm to communities and that are difficult to reverse, dismantle or even reform.















**Bibliography**
Deeks, Ashley. “Predicting Enemies.” Virginia Law Review, vol. 104, no. 8, https://www.jstor.org/stable/10.2307/26790717.
Christin, Angele. “Predictive Algorithms and Criminal Sentencing.” The Decisionist Imagination, Berghahn Books, pp. 272–94, https://www.jstor.org/stable/j.ctvw04b7q.14.
Partnership on AI. “Report on the Algorithmic Risk Assessment Tools in the U.S Criminal Justice System.” Accessed May 6, 2021. https://www.partnershiponai.org/report-on-machine-learning-in-risk-assessment-tools-in-the-u-s-criminal-justice-system/.
Larson, Jeff, Surya Mattu, Lauren Kirchner, and Julia Angwin. “How We Analyzed the Compas Recidivism Algorithm,” May 23, 2016. https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm.
Chohlas-Wood, Alex. “Understanding Risk Assessment Instruments in Criminal Justice,” 19 2020. https://www.brookings.edu/research/understanding-risk-assessment-instruments-in-criminal-justice/.
Kehl, Danielle, Priscilla Guo, and Samuel Kessler. 2017. Algorithms in the Criminal Justice System: Assessing the Use of Risk Assessments in Sentencing. Responsive Communities Initiative, Berkman Klein Center for Internet & Society, Harvard Law School.
	http://nrs.harvard.edu/urn-3:HUL.InstRepos:33746041

